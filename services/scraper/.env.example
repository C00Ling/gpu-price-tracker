# Scraper Worker Environment Variables
# Copy this file to .env and update values

# ============================================================
# Environment
# ============================================================
ENVIRONMENT=development
SERVICE_NAME=scraper

# ============================================================
# Database Configuration
# ============================================================
# Local development (Docker Compose)
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/gpu_tracker

# Railway (use Railway's DATABASE_URL variable)
# DATABASE_URL=${{Postgres.DATABASE_URL}}

# Local PostgreSQL
# DATABASE_URL=postgresql://postgres:password@localhost:5432/gpu_tracker

# ============================================================
# Worker Mode Configuration
# ============================================================
# Worker modes:
#   - daemon:  Continuous scraping with interval (recommended for production)
#   - oneshot: Single scrape and exit (for manual runs, testing)
#   - cron:    Scheduled via cron daemon
WORKER_MODE=daemon

# Scraping interval (seconds) - only used in daemon mode
# 3600  = 1 hour (development)
# 21600 = 6 hours (production default)
SCRAPE_INTERVAL_SECONDS=3600

# ============================================================
# TOR Configuration
# ============================================================
TOR_ENABLED=true
TOR_WAIT_TIME=10

# ============================================================
# Scraper Settings
# ============================================================
# Maximum pages to scrape per search term
# Development: 5 (fast testing)
# Production: 999 (scrape all pages)
SCRAPER_MAX_PAGES=5

# Logging level
LOG_LEVEL=INFO

# ============================================================
# Error Monitoring (Optional)
# ============================================================
# Get DSN from https://sentry.io
# SENTRY_DSN=https://examplePublicKey@o0.ingest.sentry.io/0
# SENTRY_ENVIRONMENT=development

# ============================================================
# Production Recommendations
# ============================================================
# For Railway deployment:
#   - WORKER_MODE=daemon
#   - SCRAPE_INTERVAL_SECONDS=21600 (6 hours)
#   - SCRAPER_MAX_PAGES=999 (all pages)
#   - TOR_ENABLED=true
#
# For manual testing:
#   - WORKER_MODE=oneshot
#   - SCRAPER_MAX_PAGES=5
#
# For cron-based scheduling:
#   - WORKER_MODE=cron
#   - Configure schedule in services/scraper/crontab

# ============================================================
# Notes
# ============================================================
# - The scraper worker runs independently from the API
# - It scrapes data and saves to the database
# - The API service reads this data (read-only)
# - Only 1 scraper instance should run to avoid duplicates
# - For local development, use docker-compose up
