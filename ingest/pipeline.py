# ingest/pipeline.py
from ingest.scraper import GPUScraper, SAMPLE_BENCHMARKS, ScraperError
from storage.db import SessionLocal, init_db
from storage.repo import GPURepository, RepositoryError
from core.logging import get_logger
from core.config import config
from collections import defaultdict
import sys

logger = get_logger("pipeline")


def run_pipeline():
    """
    Enhanced pipeline —Å—ä—Å –∑–∞—â–∏—Ç–∞ –æ—Ç –≥—Ä–µ—à–∫–∏ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ –ª–æ–≥–≤–∞–Ω–µ
    """
    logger.info("="*70)
    logger.info("üöÄ STARTING DATA COLLECTION PIPELINE")
    logger.info("="*70)
    
    try:
        # 1Ô∏è‚É£ Initialize database
        logger.info("üì¶ Initializing database...")
        init_db()
        logger.info("‚úÖ Database ready")
        
        # 2Ô∏è‚É£ Initialize scraper
        logger.info(f"üîß Initializing scraper (TOR: {config.scraper_use_tor})...")
        scraper = GPUScraper(use_tor=config.scraper_use_tor)
        scraper.add_benchmark_data(SAMPLE_BENCHMARKS)
        logger.info("‚úÖ Scraper initialized")
        
        # 3Ô∏è‚É£ Test connection
        logger.info("üîç Testing connection...")
        if not scraper.test_connection():
            logger.error("‚ùå No connection! Check if TOR is running.")
            return False
        logger.info("‚úÖ Connection test passed")
        
        # 4Ô∏è‚É£ SCRAPE: Collect ALL data (without filtering)
        logger.info("\n" + "="*70)
        logger.info("üîç SCRAPING: Collecting all data")
        logger.info("="*70)

        try:
            scraper.scrape_olx_pass(
                search_term="–≤–∏–¥–µ–æ –∫–∞—Ä—Ç–∞",
                max_pages=config.scraper_max_pages,
                apply_filters=False  # No filtering during scrape
            )
            raw_total = sum(len(v) for v in scraper.gpu_prices.values())
            logger.info(
                f"‚úÖ Scraping complete: {len(scraper.gpu_prices)} models, "
                f"{raw_total} total listings (unfiltered)"
            )
        except ScraperError as e:
            logger.error(f"‚ùå Scraping failed: {e}")
            return False

        # 5Ô∏è‚É£ POST-PROCESSING: Filter scraped data
        logger.info("\n" + "="*70)
        logger.info("üßπ POST-PROCESSING: Filtering data")
        logger.info("="*70)

        from core.filters import filter_scraped_data

        try:
            filtered_data, filter_stats = filter_scraped_data(scraper.gpu_prices)
            filtered_total = filter_stats['total_kept']

            logger.info(f"‚úÖ Filtering complete:")
            logger.info(f"   Total listings (raw):      {raw_total}")
            logger.info(f"   Filtered out (outliers):   {filter_stats['total_filtered']}")
            logger.info(f"   - Extremely low price:     {filter_stats['extremely_low_price']}")
            logger.info(f"   - Statistical outlier low: {filter_stats['statistical_outlier_low']}")
            logger.info(f"   - Statistical outlier high:{filter_stats['statistical_outlier_high']}")
            logger.info(f"   Kept (valid):              {filtered_total}")
            logger.info(f"   Filter rate:               {(filter_stats['total_filtered'] / raw_total * 100):.1f}%")

            # Replace raw data with filtered data
            scraper.gpu_prices = defaultdict(list, filtered_data)

        except Exception as e:
            logger.error(f"‚ùå Post-processing failed: {e}")
            return False

        # 6Ô∏è‚É£ Save to database (only filtered data)
        logger.info("\nüíæ Saving to database...")

        try:
            session = SessionLocal()
            repo = GPURepository(session)

            # Clear old data
            logger.info("üóëÔ∏è  Clearing old data...")
            repo.clear_listings()
            
            # Save new data using bulk insert
            listings = []
            for model, prices in scraper.gpu_prices.items():
                for price in prices:
                    listings.append({
                        'model': model,
                        'source': 'OLX',
                        'price': price
                    })
            
            total_saved = repo.add_listings_bulk(listings)
            session.close()
            
            logger.info(f"‚úÖ Saved {total_saved} listings to database")
            
        except RepositoryError as e:
            logger.error(f"‚ùå Database save failed: {e}")
            return False
        
        # 8Ô∏è‚É£ Display statistics
        logger.info("\n" + "="*70)
        logger.info("üìä STATISTICS BY MODEL")
        logger.info("="*70)
        
        try:
            session = SessionLocal()
            repo = GPURepository(session)
            
            models = sorted(repo.get_models())
            
            if not models:
                logger.warning("‚ö†Ô∏è  No models found in database")
            else:
                logger.info(f"{'Model':<20} | {'Count':<5} | {'Min':<8} | {'Median':<8} | {'Max':<8}")
                logger.info("-" * 70)
                
                for model in models:
                    stats = repo.get_price_stats(model)
                    if stats:
                        logger.info(
                            f"{model:<20} | "
                            f"{stats['count']:<5} | "
                            f"{stats['min']:>6.0f}–ª–≤ | "
                            f"{stats['median']:>6.0f}–ª–≤ | "
                            f"{stats['max']:>6.0f}–ª–≤"
                        )
            
            session.close()
            
        except Exception as e:
            logger.error(f"Error displaying statistics: {e}")
        
        # 9Ô∏è‚É£ Success!
        logger.info("\n" + "="*70)
        logger.info("‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
        logger.info("="*70)
        logger.info("")
        logger.info("üåê Start API server with: uvicorn main:app --reload")
        logger.info("üìñ API docs: http://127.0.0.1:8000/docs")
        logger.info("üé® Dashboard: http://127.0.0.1:8000/dashboard")
        logger.info("")
        
        return True
        
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Pipeline interrupted by user")
        return False
        
    except Exception as e:
        logger.error(f"\n‚ùå PIPELINE FAILED: {e}", exc_info=True)
        return False


if __name__ == "__main__":
    success = run_pipeline()
    sys.exit(0 if success else 1)