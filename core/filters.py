"""
Enhanced quality filters for GPU listings - Post-Processing Statistical Filtering

This module implements post-processing filtering:
1. SCRAPE: Collect ALL data without filtering
2. POST-PROCESS: Apply statistical outlier detection after scraping is complete

Advantages:
- No warm-up phase needed - we have full statistics before filtering
- More accurate outlier detection with complete dataset
- Easier to debug and tune thresholds
"""
from typing import Tuple, Optional, List, Dict, Any
import re
from core.logging import get_logger
from core.config import config

logger = get_logger("filters")

# Expanded blacklist keywords
BLACKLIST_KEYWORDS = [
    # Bulgarian
    "–∑–∞ —á–∞—Å—Ç–∏", "—Å—á—É–ø–µ–Ω–∞", "–Ω–µ —Ä–∞–±–æ—Ç–∏", "–ø–æ–≤—Ä–µ–¥–µ–Ω–∞", "–¥–µ—Ñ–µ–∫—Ç",
    "–∑–∞ —Ä–µ–º–æ–Ω—Ç", "–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∏", "—á–µ—Ä–µ–Ω –µ–∫—Ä–∞–Ω", "–Ω–µ –¥–∞–≤–∞ –µ–∫—Ä–∞–Ω", "–Ω–µ —Å—Ç–∞—Ä—Ç–∏—Ä–∞", "–∏–∑–≥–æ—Ä—è",
    "—Ä–∞–∑–≤–∞–ª–µ–Ω", "–Ω–µ—Ç–µ—Å—Ç–≤–∞–Ω–∞", "–ø—Ä–æ–±–ª–µ–º", "–Ω–µ –µ —Ç–µ—Å—Ç–≤–∞–Ω–∞", "–¥–µ—Ñ–µ–∫—Ç–Ω–∞", "–Ω–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–∞", "–∏–∑–ø—Ä–∞–≤–Ω–∏",
    "–Ω—è–º–∞ —Å–∏–≥–Ω–∞–ª", "–±–µ–∑ —Å–∏–≥–Ω–∞–ª", "–Ω–µ –¥–∞–≤–∞ —Å–∏–≥–Ω–∞–ª",

    # Mining-related (often worn out)
    "–º–∞–π–Ω–∏–Ω–≥", "mining", "burnout", "mining rig", "–∫–æ–ø–∞–Ω–∞", "—Ñ–µ—Ä–º–∞", "mining farm",
    "–æ—Ç —Ñ–µ—Ä–º–∞", "–æ—Ç –º–∞–π–Ω–∏–Ω–≥", "–∑–∞ –º–∞–π–Ω–∏–Ω–≥", "–∫–æ–ø–∞–µ–Ω–µ",

    # English
    "broken", "damaged", "faulty", "defective", "not working", "for parts",
    "parts only", "as is", "repair", "artifacts", "black screen",
    "burnt", "dead", "fried", "doa", "no signal", "no display",

    # Common suspicious patterns
    "—Å—Ä–æ—á–Ω–æ", "–±—ä—Ä–∑–æ", "—Å–ø–µ—à–Ω–æ",  # Often scams
]

# Outlier detection thresholds
OUTLIER_THRESHOLD_LOW = 0.50   # 50% –æ—Ç –º–µ–¥–∏–∞–Ω–∞—Ç–∞ (–ø–æ-–±–∞–ª–∞–Ω—Å–∏—Ä–∞–Ω–æ —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ)
OUTLIER_THRESHOLD_HIGH = 3.0   # 300% –æ—Ç –º–µ–¥–∏–∞–Ω–∞—Ç–∞ (–∑–∞ —Å–∫—ä–ø–∏ outliers)

# Minimum sample size –∑–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
MIN_SAMPLE_SIZE = 3  # –ú–∏–Ω–∏–º—É–º 3 –æ–±—è–≤–∏ –∑–∞ –¥–∞ –ø—Ä–∏–ª–æ–∂–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–ë–ï–ó warm-up —Ñ–∞–∑–∞)
ADAPTIVE_WARMUP_SIZE = 5  # –°–ª–µ–¥ 5 –æ–±—è–≤–∏ –∏–∑–ø–æ–ª–∑–≤–∞–º–µ –ø—ä–ª–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞ —Ñ–∏–ª—Ç—Ä–∞—Ü–∏—è


# Model corrections for incomplete/ambiguous names
MODEL_CORRECTIONS = {
    # AMD RX 7000-series - fix incomplete model names
    "RX 7900": "RX 7900 XT",      # Default to XT (most common)
    "RX 7800": "RX 7800 XT",      # Only XT variant exists
    "RX 7700": "RX 7700 XT",      # Only XT variant exists
    "RX 7600": "RX 7600",         # Non-XT is the base model

    # AMD RX 6000-series - fix incomplete model names
    "RX 6950": "RX 6950 XT",      # Only XT variant exists
    "RX 6900": "RX 6900 XT",      # Only XT variant exists
    "RX 6800": "RX 6800",         # Non-XT is valid
    "RX 6700": "RX 6700 XT",      # XT is more common
    "RX 6600": "RX 6600",         # Non-XT is the base model
    "RX 6500": "RX 6500 XT",      # Only XT variant exists

    # AMD RX 5000-series
    "RX 5700": "RX 5700 XT",      # XT is more common
    "RX 5600": "RX 5600 XT",      # Only XT variant exists
    "RX 5500": "RX 5500 XT",      # Only XT variant exists

    # NVIDIA RTX 40-series
    "RTX 4090": "RTX 4090",       # Only non-SUPER exists
    "RTX 4080": "RTX 4080 SUPER", # SUPER is newer/better
    "RTX 4070": "RTX 4070 SUPER", # SUPER is more common now
    "RTX 4060": "RTX 4060 TI",    # TI is more common

    # NVIDIA RTX 30-series
    "RTX 3090": "RTX 3090",       # Non-TI is valid
    "RTX 3080": "RTX 3080",       # Non-TI is valid
    "RTX 3070": "RTX 3070",       # Non-TI is valid
    "RTX 3060": "RTX 3060 TI",    # TI is more common
    "RTX 3050": "RTX 3050",       # Only non-8GB exists

    # NVIDIA RTX 20-series
    "RTX 2080": "RTX 2080 SUPER", # SUPER is more common
    "RTX 2070": "RTX 2070 SUPER", # SUPER is more common
    "RTX 2060": "RTX 2060 SUPER", # SUPER is more common

    # NVIDIA GTX 16-series
    "GTX 1660": "GTX 1660 SUPER", # SUPER is more common
    "GTX 1650": "GTX 1650 SUPER", # SUPER is more common

    # NVIDIA GTX 10-series
    "GTX 1080": "GTX 1080 TI",    # TI is more common
    "GTX 1070": "GTX 1070 TI",    # TI is more common
    "GTX 1060": "GTX 1060 6GB",   # 6GB is more common
    "GTX 1050": "GTX 1050 TI",    # TI is more common

    # Common typos and errors
    "GTX 1060 SUPER": "GTX 1660 SUPER",  # Common confusion
    "GTX 1600": "GTX 1650 SUPER",        # Typo
    "RTX 2260 SUPER": "RTX 2060 SUPER",  # Typo
    "RX 1650": "GTX 1650 SUPER",         # Brand confusion (AMD ‚Üí NVIDIA)
}


def normalize_model_name(model: str) -> str:
    """
    Normalize GPU model name for consistency

    Examples:
        RTX3060TI -> RTX 3060 TI
        RTX 3060TI -> RTX 3060 TI
        RX6600XT -> RX 6600 XT
        VEGA56 -> VEGA 56
        gtx 1660ti -> GTX 1660 TI
        GTX 1060 6GB -> GTX 1060 6GB
        RX 7900 -> RX 7900 XT (autocorrect incomplete names)
        AMD Radeon RX 7900 GRE -> RX 7900 GRE
    """
    if not model:
        return model

    # Convert to uppercase
    model = model.upper().strip()

    # Remove brand prefixes (AMD, NVIDIA, GEFORCE, RADEON, INTEL) - with optional spaces
    # This must run BEFORE removing all spaces
    model = re.sub(r'^(AMD|NVIDIA|GEFORCE|RADEON|INTEL)\s+', '', model)

    # Remove "RADEON" if it appears after removing first prefix (e.g., "AMD RADEON RX")
    model = re.sub(r'^RADEON\s+', '', model)
    model = re.sub(r'^GEFORCE\s+', '', model)

    # Remove all remaining spaces
    model = model.replace(" ", "")

    # Add space after brand (RTX, GTX, RX, VEGA, ARC)
    model = re.sub(r'(RTX|GTX|RX|VEGA|ARC)(\d+)', r'\1 \2', model)

    # Add space before memory size (3GB, 6GB, 8GB, 12GB, 16GB, etc.) - FIRST
    # This must run before TI/SUPER/XT to handle cases like "TI16GB"
    model = re.sub(r'(\d{4})(\d{1,2}GB)$', r'\1 \2', model)  # e.g., 30603GB -> 3060 3GB
    model = re.sub(r'(TI|SUPER|XT|XTX|GRE)(\d{1,2}GB)$', r'\1 \2', model)  # e.g., TI16GB -> TI 16GB

    # Add space before suffix (TI, SUPER, XT, XTX, GRE)
    model = re.sub(r'(\d+)(TI|SUPER|XT|XTX|GRE)', r'\1 \2', model)

    # Apply model corrections for incomplete/ambiguous names
    if model in MODEL_CORRECTIONS:
        corrected = MODEL_CORRECTIONS[model]
        logger.debug(f"Model correction: '{model}' ‚Üí '{corrected}'")
        model = corrected

    return model


def is_suspicious_listing(
    title: str,
    price: float,
    gpu_model: str,
    dynamic_min_price: int = 0,
    all_prices_for_model: Optional[List[float]] = None
) -> Tuple[bool, str]:
    """
    DEPRECATED: This function is no longer used for real-time filtering.

    Filtering now happens in post-processing via filter_scraped_data().
    This function is kept for backwards compatibility only.

    Args:
        title: Listing title
        price: Price in BGN
        gpu_model: GPU model name
        dynamic_min_price: Deprecated (kept for backwards compatibility)
        all_prices_for_model: Deprecated (kept for backwards compatibility)

    Returns:
        Tuple of (is_suspicious: bool, reason: str)
    """
    title_lower = title.lower()

    # Normalize the model for comparison
    gpu_model = normalize_model_name(gpu_model)

    # 1. Check for blacklisted keywords (ALWAYS APPLIED - HIGHEST PRIORITY)
    for keyword in BLACKLIST_KEYWORDS:
        if keyword.lower() in title_lower:
            return (True, f"Contains blacklisted keyword: '{keyword}'")

    # 2. Extremely low price check (ALWAYS APPLIED - universal red flag)
    if price < 50:
        return (True, f"Extremely low price: {price}–ª–≤ (likely broken)")

    # 3. Title length check (ALWAYS APPLIED - low quality listings)
    if len(title) < 10:
        return (True, f"Title too short: '{title}'")

    # 4. ADAPTIVE Statistical outlier detection
    # Only apply if we have enough samples (warm-up phase complete)
    if all_prices_for_model and len(all_prices_for_model) >= ADAPTIVE_WARMUP_SIZE:
        import statistics

        median = statistics.median(all_prices_for_model)

        # Check if price is too low (outlier)
        low_threshold = median * OUTLIER_THRESHOLD_LOW
        if price < low_threshold:
            return (
                True,
                f"Statistical outlier: {price}–ª–≤ < {low_threshold:.0f}–ª–≤ "
                f"(30% of median {median:.0f}–ª–≤)"
            )

        # Check if price is too high (probably scam/wrong listing)
        high_threshold = median * OUTLIER_THRESHOLD_HIGH
        if price > high_threshold:
            return (
                True,
                f"Price too high: {price}–ª–≤ > {high_threshold:.0f}–ª–≤ "
                f"(300% of median {median:.0f}–ª–≤)"
            )

    # All checks passed
    return (False, "OK")


def calculate_statistics(prices: List[float]) -> Optional[Dict[str, Any]]:
    """
    Calculate statistics for a list of prices
    
    Args:
        prices: List of prices
    
    Returns:
        Dict with median, mean, std_dev, q1, q3, iqr
        None if not enough data
    """
    if not prices or len(prices) < 2:
        return None
    
    import statistics
    
    sorted_prices = sorted(prices)
    n = len(sorted_prices)
    
    stats = {
        'median': statistics.median(sorted_prices),
        'mean': statistics.mean(sorted_prices),
        'count': n,
    }
    
    # Standard deviation (if enough data)
    if n >= 2:
        stats['std_dev'] = statistics.stdev(sorted_prices)
    
    # Quartiles (if enough data)
    if n >= 4:
        stats['q1'] = sorted_prices[n // 4]
        stats['q3'] = sorted_prices[3 * n // 4]
        stats['iqr'] = stats['q3'] - stats['q1']
    
    return stats


def filter_scraped_data(raw_data: Dict[str, List[float]]) -> tuple[Dict[str, List[float]], Dict[str, int]]:
    """
    Post-processing filtering: —Ñ–∏–ª—Ç—Ä–∏—Ä–∞ scraped –¥–∞–Ω–Ω–∏ –°–õ–ï–î —Å—ä–±–∏—Ä–∞–Ω–µ—Ç–æ –∏–º

    –ü—Ä–µ–¥–∏–º—Å—Ç–≤–∞:
    - –ò–º–∞–º–µ –ø—ä–ª–Ω–∞—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥–∏ —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ (–Ω—è–º–∞ warm-up —Ñ–∞–∑–∞)
    - –ü–æ-—Ç–æ—á–Ω–æ outlier detection
    - –ü–æ-–ø—Ä–æ—Å—Ç–æ –∑–∞ debugging

    Args:
        raw_data: –†–µ—á–Ω–∏–∫ {model: [prices]} —Å—ä—Å –í–°–ò–ß–ö–ò scraped –¥–∞–Ω–Ω–∏

    Returns:
        Tuple of (filtered_data, filter_stats)
        - filtered_data: –†–µ—á–Ω–∏–∫ {model: [prices]} —Å–∞–º–æ —Å –≤–∞–ª–∏–¥–Ω–∏ —Ü–µ–Ω–∏
        - filter_stats: –†–µ—á–Ω–∏–∫ {reason: count} —Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ —Ñ–∏–ª—Ç—Ä–∏—Ä–∞–Ω–µ
    """
    filtered_data = {}
    filter_stats = {
        'blacklist_keywords': 0,
        'extremely_low_price': 0,
        'statistical_outlier_low': 0,
        'statistical_outlier_high': 0,
        'total_filtered': 0,
        'total_kept': 0,
    }

    for model, prices in raw_data.items():
        if not prices or len(prices) < MIN_SAMPLE_SIZE:
            # Keep models with too few listings (no stats available)
            filtered_data[model] = prices
            filter_stats['total_kept'] += len(prices)
            continue

        import statistics
        median = statistics.median(prices)
        low_threshold = median * OUTLIER_THRESHOLD_LOW
        high_threshold = median * OUTLIER_THRESHOLD_HIGH

        valid_prices = []
        for price in prices:
            # Check extremely low price (< 50 –ª–≤)
            if price < 50:
                filter_stats['extremely_low_price'] += 1
                filter_stats['total_filtered'] += 1
                logger.debug(f"Filtered {model} @ {price}–ª–≤: extremely low price")
                continue

            # Check low outlier
            if price < low_threshold:
                filter_stats['statistical_outlier_low'] += 1
                filter_stats['total_filtered'] += 1
                logger.debug(
                    f"Filtered {model} @ {price}–ª–≤: < {low_threshold:.0f}–ª–≤ "
                    f"(50% of median {median:.0f}–ª–≤)"
                )
                continue

            # Check high outlier
            if price > high_threshold:
                filter_stats['statistical_outlier_high'] += 1
                filter_stats['total_filtered'] += 1
                logger.debug(
                    f"Filtered {model} @ {price}–ª–≤: > {high_threshold:.0f}–ª–≤ "
                    f"(300% of median {median:.0f}–ª–≤)"
                )
                continue

            # Price passed all checks
            valid_prices.append(price)
            filter_stats['total_kept'] += 1

        if valid_prices:
            filtered_data[model] = valid_prices

    return filtered_data, filter_stats


def get_filter_summary(filtered_data: Dict[str, List[float]]) -> str:
    """
    Generate a summary of post-processing filtering results

    Args:
        filtered_data: Dict of {model: [prices]} AFTER filtering

    Returns:
        Formatted string with filtering info
    """
    summary = []
    summary.append("üìä Post-Processing Filter Results:")
    summary.append(f"  Low Outlier Threshold:  < {OUTLIER_THRESHOLD_LOW * 100:.0f}% of median")
    summary.append(f"  High Outlier Threshold: > {OUTLIER_THRESHOLD_HIGH * 100:.0f}% of median")
    summary.append(f"  Min Sample Size:        {MIN_SAMPLE_SIZE} listings")
    summary.append("")

    for model, prices in sorted(filtered_data.items()):
        if len(prices) >= MIN_SAMPLE_SIZE:
            import statistics
            median = statistics.median(prices)
            low = median * OUTLIER_THRESHOLD_LOW
            high = median * OUTLIER_THRESHOLD_HIGH
            summary.append(
                f"  {model:20} ‚Üí {low:>5.0f}–ª–≤ - {high:>6.0f}–ª–≤ "
                f"(median: {median:.0f}–ª–≤, n={len(prices)})"
            )
        else:
            summary.append(
                f"  {model:20} ‚Üí No filtering (n={len(prices)} < {MIN_SAMPLE_SIZE})"
            )

    return "\n".join(summary)