# Railway Configuration for Scraper Worker
# Autonomous cron-based worker for GPU data collection

[build]
builder = "dockerfile"
dockerfilePath = "services/scraper/Dockerfile"
watchPatterns = [
    "services/scraper/**",
    "services/shared/**",
    "config/**"
]

[deploy]
startCommand = "./start.sh"
restartPolicyType = "always"  # Always restart worker
restartPolicyMaxRetries = 10

# No HTTP healthcheck (worker doesn't expose HTTP)
# Instead we rely on process healthcheck

[env]
# Service identification
SERVICE_NAME = "scraper"
ENVIRONMENT = "production"

# Worker mode: oneshot | daemon | cron
WORKER_MODE = "daemon"  # For Railway, use daemon mode

# Scraping interval (in seconds) - 6 hours
SCRAPE_INTERVAL_SECONDS = "21600"  # 6 hours

# TOR configuration
TOR_ENABLED = "true"
TOR_WAIT_TIME = "10"

# Scraper configuration
SCRAPER_MAX_PAGES = "999"  # Scrape all pages
SCRAPER_RATE_LIMIT = "8"   # 8 requests per minute
SCRAPER_TIMEOUT = "30"     # 30 seconds timeout

# Logging
LOG_LEVEL = "INFO"
LOG_FORMAT = "json"

# NOTE: Database credentials should be added via Railway UI:
# - DATABASE_URL (from PostgreSQL service)
# - SENTRY_DSN (optional, for error monitoring)

[healthcheck]
enabled = false  # No HTTP healthcheck for worker

[resources]
# Scraper worker resource limits
memory = 1024  # MB (needs more memory for scraping)
cpu = 1.0      # CPU cores

[scaling]
# Worker should NOT scale horizontally (only 1 instance)
minReplicas = 1
maxReplicas = 1  # Fixed at 1 to avoid duplicate scraping
